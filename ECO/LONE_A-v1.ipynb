{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "HIDDEN_LAYER        = 50\n",
    "BATCH_SIZE          = 32\n",
    "LR                  = 1e-4  # learning rate\n",
    "EPSILON             = 0.9   # greedy policy\n",
    "GAMMA               = 0.9   # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY     = 100000\n",
    "TERMINAL_BIAS       = 0.5   # no. of terminal memories in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTRA_FEAT   = 1 #masscart, masspole, length\n",
    "N_ACTIONS   = env.action_space.n \n",
    "N_STATES    = env.observation_space.shape[0] + XTRA_FEAT\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.adv = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.adv.weight) \n",
    "    \n",
    "        self.val = nn.Linear(HIDDEN_LAYER, 1)\n",
    "        nn.init.xavier_uniform_(self.val.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        adv = self.adv(x)\n",
    "        val = self.val(x)\n",
    "        \n",
    "        return val + adv - adv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "        \n",
    "        self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "        self.good_memory         = np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "        self.bad_memory          = np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        if r > 0: #non-terminal rewards\n",
    "            # replace the old memory with new memory\n",
    "            index = self.good_memory_counter % int(MEMORY_CAPACITY/2)\n",
    "            self.good_memory[index, :] = transition\n",
    "            self.good_memory_counter += 1\n",
    "        \n",
    "        else: #terminal rewards\n",
    "            # replace the old memory with new memory\n",
    "            index = self.bad_memory_counter % int(MEMORY_CAPACITY/2)\n",
    "            self.bad_memory[index, :] = transition\n",
    "            self.bad_memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        # sample batch transitions\n",
    "        good_sample_index_limit = min(MEMORY_CAPACITY/2, self.good_memory_counter)\n",
    "        bad_sample_index_limit  = min(MEMORY_CAPACITY/2, self.bad_memory_counter)\n",
    "        \n",
    "        good_sample_index = np.random.choice(int(good_sample_index_limit), int(BATCH_SIZE-int(BATCH_SIZE*TERMINAL_BIAS)))\n",
    "        bad_sample_index  = np.random.choice(int(bad_sample_index_limit),  int(BATCH_SIZE*TERMINAL_BIAS))\n",
    "\n",
    "        b_good_memory = self.good_memory[good_sample_index, :]\n",
    "        b_bad_memory  = self.bad_memory[bad_sample_index, :]\n",
    "        b_memory      = np.vstack((b_good_memory,b_bad_memory))\n",
    "        \n",
    "        b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(BATCH_SIZE, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = D3QN()\n",
    "NO_OF_EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n",
      "501\n",
      "Learning starts from EPISODE:  501\n"
     ]
    }
   ],
   "source": [
    "print('\\nCollecting experience...')\n",
    "TIMESTEP_LIMIT = 200\n",
    "total_time_steps = 0\n",
    "time_rec = []\n",
    "learn_start_flag = True\n",
    "learn_start = 0\n",
    "solve_metric = 195\n",
    "upgrade_flag = False\n",
    "upgrade_counter = 0\n",
    "state_max  = np.ones(N_STATES)*(-100)\n",
    "state_min  = np.ones(N_STATES)*(100)\n",
    "for i_episode in range(NO_OF_EPISODES):\n",
    "    env.length   = 0.5 + np.random.uniform(-0.3,0.3)\n",
    "    \n",
    "    s = env.reset()\n",
    "    s = np.append(s, [env.length])\n",
    "    ep_r = 0\n",
    "    time_steps = 0\n",
    "    while True:\n",
    "#         env.render()\n",
    "        state_max = np.maximum(state_max,s) \n",
    "        state_min = np.minimum(state_min,s)\n",
    "        time_steps += 1\n",
    "        total_time_steps += 1\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.append(s_, [env.length])\n",
    "\n",
    "        if done:\n",
    "            if time_steps >= TIMESTEP_LIMIT:\n",
    "                r = 1\n",
    "            else:\n",
    "                r = -1\n",
    "           \n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "        \n",
    "        if dqn.bad_memory_counter > (500):\n",
    "            dqn.learn()\n",
    "            if learn_start_flag:\n",
    "                print(i_episode)\n",
    "                print(\"Learning starts from EPISODE: \",i_episode)\n",
    "                learn_start = i_episode\n",
    "                learn_start_flag = False\n",
    "        \n",
    "        if done or time_steps >= TIMESTEP_LIMIT:\n",
    "            time_rec = np.append(time_rec, time_steps)\n",
    "            break\n",
    "        s = s_\n",
    "    \n",
    "    #if minimum of episode length of last 100 episodes is greater than upgrade_metric=195\n",
    "    if time_rec[-100:].min() > solve_metric:\n",
    "        print(\"Solved\")\n",
    "        break\n",
    "#         upgrade_counter += 1\n",
    "#     else:\n",
    "#         upgrade_counter = 0\n",
    "        \n",
    "#     if upgrade_counter > 110:\n",
    "#         upgrade_counter = 0\n",
    "#         print(\"Upgrading @ EPISODE: \", i_episode)\n",
    "# #         upgrade_metric *= 2\n",
    "# #         LR *= 0.1\n",
    "#         EPSILON += 0.02\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,6))\n",
    "\n",
    "ax2 = fig.add_subplot(2, 1, 1)\n",
    "data = time_rec\n",
    "ax2.plot(data, color = 'g')\n",
    "ax2.plot(np.ones_like(data)*1500, 'g--')\n",
    "ax2.plot(np.ones_like(data)*200, 'r--')\n",
    "\n",
    "ax2.set_xlabel('Iterations',color = 'k')\n",
    "ax2.set_ylabel('Time Steps',color = 'g')\n",
    "ax2.set_ylim([1,2.5e2])\n",
    "fig.tight_layout()\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 100\n",
    "# ravg = np.convolve(time_rec, np.ones((WINDOW,))/WINDOW, mode='valid')\n",
    "ravg = running_mean(time_rec, WINDOW)\n",
    "plt.plot(ravg)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = './models/cartpole_v0-custom_xtra' + datetime.now().strftime(\"_%H_%M_%S\")\n",
    "print(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn.eval_net.state_dict(), MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_max)\n",
    "print(state_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOTAL TIMESTEPS: \", total_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_rec = []\n",
    "for i_episode in range(1000):\n",
    "    env.length   = 0.5 + np.random.uniform(-0.3,0.3)\n",
    "    xtra = [env.length]\n",
    "    s = env.reset()\n",
    "    s = np.append(s, xtra)\n",
    "    time_steps = 0\n",
    "    while True:\n",
    "#         env.render()\n",
    "        time_steps += 1\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.append(s_, xtra)\n",
    "\n",
    "        if done or time_steps >= TIMESTEP_LIMIT:\n",
    "            test_time_rec = np.append(test_time_rec, time_steps)\n",
    "            break\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,6))\n",
    "\n",
    "ax2 = fig.add_subplot(2, 1, 1)\n",
    "data = test_time_rec\n",
    "ax2.plot(data, color = 'g')\n",
    "ax2.plot(np.ones_like(data)*1500, 'g--')\n",
    "ax2.plot(np.ones_like(data)*200, 'r--')\n",
    "\n",
    "ax2.set_xlabel('Iterations',color = 'k')\n",
    "ax2.set_ylabel('Time Steps',color = 'g')\n",
    "ax2.set_ylim([1,2.5e2])\n",
    "fig.tight_layout()\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
